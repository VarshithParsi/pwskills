{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e0765c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4848bce0",
   "metadata": {},
   "source": [
    "1Q) What is a Decision Tree, and how does it work in the context of classification?\n",
    "\n",
    "Ans) A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In classification, it works by splitting the dataset into subsets based on the value of input features. Each internal node represents a decision on a feature, each branch represents the outcome of the decision, and each leaf node represents a class label. The tree is built by selecting the best feature that splits the data to maximize class purity, often using metrics like Gini Impurity or Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31563da",
   "metadata": {},
   "source": [
    "2Q) Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
    "\n",
    "Ans)\n",
    "Gini Impurity and Entropy are impurity measures used in Decision Trees to evaluate how well a feature separates the data into classes.\n",
    "\n",
    "Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "Formula:\n",
    "Gini = 1 − Σ (pᵢ)²\n",
    "Where:\n",
    "\n",
    "pᵢ is the probability of class i in the node.\n",
    "Entropy measures the amount of uncertainty or randomness in the data.\n",
    "\n",
    "Formula:\n",
    "Entropy = − Σ pᵢ × log₂(pᵢ)\n",
    "Where:\n",
    "\n",
    "pᵢ is the probability of class i in the node.\n",
    "Impact on Splits:\n",
    "\n",
    "Lower Gini or Entropy values indicate purer nodes.\n",
    "The Decision Tree algorithm chooses the feature that results in the greatest reduction in impurity (i.e., highest Information Gain).\n",
    "Gini is computationally faster, while Entropy can be more informative in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d1a04",
   "metadata": {},
   "source": [
    "3Q) What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "\n",
    "Ans)\n",
    "Pre-Pruning stops the tree from growing once a condition is met (e.g., max depth, min samples).\n",
    "Advantage: Reduces overfitting early and improves training speed.\n",
    "Post-Pruning allows the tree to grow fully and then removes branches that do not improve performance.\n",
    "Advantage: Often results in better generalization by evaluating actual performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9761f",
   "metadata": {},
   "source": [
    "4Q) What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "\n",
    "Ans)\n",
    "Information Gain (IG) is a metric used to measure how well a feature separates the training examples according to their target classes. It is based on the concept of Entropy, which quantifies the impurity or disorder in a dataset.\n",
    "\n",
    "Formula:\n",
    "Information Gain = Entropy(Parent) − Weighted Average Entropy(Children)\n",
    "\n",
    "Mathematically:\n",
    "IG(D, A) = Entropy(D) − Σ (|Dᵥ| / |D|) × Entropy(Dᵥ)\n",
    "\n",
    "Where:\n",
    "\n",
    "D is the dataset\n",
    "A is the attribute\n",
    "Dᵥ is the subset of D where attribute A has value v\n",
    "|Dᵥ| / |D| is the proportion of samples in subset Dᵥ\n",
    "Importance:\n",
    "\n",
    "Information Gain helps in selecting the attribute that results in the most significant reduction in impurity.\n",
    "A higher Information Gain means a better split, leading to more accurate and efficient Decision Trees.\n",
    "It ensures that the tree focuses on the most informative features first, improving both performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edcf56",
   "metadata": {},
   "source": [
    "5Q) What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "\n",
    "Ans)\n",
    "Applications: Medical diagnosis, credit scoring, customer segmentation, fraud detection.\n",
    "Advantages: Easy to interpret, handle both numerical and categorical data, require little data preprocessing.\n",
    "Limitations: Prone to overfitting, sensitive to small data changes, can create biased trees if not balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b68c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Feature Importances: [0.03822004 0.         0.40445656 0.5573234 ]\n"
     ]
    }
   ],
   "source": [
    "#6Q) Write a Python program to:\n",
    "\n",
    "# Load the Iris Dataset\n",
    "# Train a Decision Tree Classifier using the Gini criterion\n",
    "# Print the model’s accuracy and feature importances\n",
    "\n",
    "#Ans)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Feature Importances:\", clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da03f3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Accuracy: 1.0\n",
      "Limited Tree Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 7Q) Write a Python program to:\n",
    "\n",
    "# Load the Iris Dataset\n",
    "# Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
    "# Ans)\n",
    "\n",
    "clf_full = DecisionTreeClassifier()\n",
    "clf_limited = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "clf_full.fit(X_train, y_train)\n",
    "clf_limited.fit(X_train, y_train)\n",
    "\n",
    "print(\"Full Tree Accuracy:\", accuracy_score(y_test, clf_full.predict(X_test)))\n",
    "print(\"Limited Tree Accuracy:\", accuracy_score(y_test, clf_limited.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ed0b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5305526897620639\n",
      "Feature Importances: [0.52368989 0.05217861 0.04925763 0.02639407 0.03200267 0.13861829\n",
      " 0.0898093  0.08804955]\n"
     ]
    }
   ],
   "source": [
    "# 8Q) Write a Python program to:\n",
    "\n",
    "# Load the California Housing dataset from sklearn\n",
    "# Train a Decision Tree Regressor\n",
    "# Print the Mean Squared Error (MSE) and feature importances\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Feature Importances:\", reg.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41d1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#9Q) Write a Python program to:\n",
    "\n",
    "# Load the Iris Dataset\n",
    "# Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "# Print the best parameters and the resulting model accuracy\n",
    "# Ans)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9109c08",
   "metadata": {},
   "source": [
    "10Q) Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
    "\n",
    "Handle the missing values\n",
    "Encode the categorical features\n",
    "Train a Decision Tree model\n",
    "Tune its hyperparameters\n",
    "Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world setting.\n",
    "\n",
    "Ans)\n",
    "Handle Missing Values: Use imputation (mean for numerical, mode for categorical) or drop rows/columns with excessive missing data.\n",
    "Encode Categorical Features: Use One-Hot Encoding or Label Encoding depending on the algorithm and feature cardinality.\n",
    "Train Model: Use DecisionTreeClassifier from sklearn with default or initial parameters.\n",
    "Tune Hyperparameters: Use GridSearchCV to optimize max_depth, min_samples_split, etc.\n",
    "Evaluate Performance: Use metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "Business Value: Helps in early disease detection, reduces manual diagnosis time, improves patient outcomes, and optimizes resource allocation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
